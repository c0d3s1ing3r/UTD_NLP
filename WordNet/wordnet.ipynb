{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple sentence describing wordnet\n",
    "text = \"WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser. WordNet is also freely and publicly available for download. WordNet's structure makes it a useful tool for computational linguistics and natural language processing.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('database.n.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select an example noun from the text\n",
    "example_noun = 'database'\n",
    "# get the synsets for the example noun\n",
    "synsets = wn.synsets(example_noun)\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('word.n.01'),\n",
       " Synset('word.n.02'),\n",
       " Synset('news.n.01'),\n",
       " Synset('word.n.04'),\n",
       " Synset('discussion.n.02'),\n",
       " Synset('parole.n.01'),\n",
       " Synset('word.n.07'),\n",
       " Synset('son.n.02'),\n",
       " Synset('password.n.01'),\n",
       " Synset('bible.n.01'),\n",
       " Synset('give_voice.v.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alright, maybe one with more synsets\n",
    "example_noun = 'word'\n",
    "synsets = wn.synsets(example_noun)\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a brief statement\n",
      "[\"he didn't say a word about it\"]\n",
      "['word']\n"
     ]
    }
   ],
   "source": [
    "# let's look at the second synset\n",
    "synset = synsets[1]\n",
    "\n",
    "# grab definition, usage examples, and lemmas\n",
    "print(synset.definition())\n",
    "print(synset.examples())\n",
    "print(synset.lemma_names())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet's noun organization is organized into synsets, which are groups of synonyms. Each synset has a unique name, and a unique offset. The offset is the synset's position in the WordNet database file. The name is a concatenation of the synset's offset, the part of speech, and the number of the synset. \n",
    "\n",
    "Nouns are arranged in a hierarchy, with more general concepts at the top, and more specific concepts at the bottom. The top of the hierarchy is called the root, and is represented by the synset `entity.n.01`. The root is the most general concept, and all other concepts are more specific (you can see this in the example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement.n.01\n",
      "message.n.02\n",
      "communication.n.02\n",
      "abstraction.n.06\n",
      "entity.n.01\n"
     ]
    }
   ],
   "source": [
    "# progress up the hierarchy for our chosen synset\n",
    "while synset.hypernyms():\n",
    "    synset = synset.hypernyms()[0]\n",
    "    print(synset.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Synset('statement.n.01')]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# reset our synset\n",
    "synset = synsets[1]\n",
    "# grab all the hyponyms, hypernyms, meronyms, antonyms, and holonyms\n",
    "print(synset.hyponyms())\n",
    "print(synset.hypernyms())\n",
    "print(synset.part_meronyms())\n",
    "print(synset.lemmas()[0].antonyms())\n",
    "print(synset.member_holonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('network.n.01'),\n",
       " Synset('network.n.02'),\n",
       " Synset('net.n.06'),\n",
       " Synset('network.n.04'),\n",
       " Synset('network.n.05'),\n",
       " Synset('network.v.01')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not a great example, let's try a different example noun\n",
    "example_noun = 'network'\n",
    "synsets = wn.synsets(example_noun)\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('espionage_network.n.01'), Synset('old_boy_network.n.01'), Synset('reticulum.n.02'), Synset('support_system.n.01')]\n",
      "[Synset('system.n.02')]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "synset = synsets[0]\n",
    "print(synset.hyponyms())\n",
    "print(synset.hypernyms())\n",
    "print(synset.part_meronyms())\n",
    "print(synset.lemmas()[0].antonyms())\n",
    "print(synset.member_holonyms())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('express.v.01'),\n",
       " Synset('express.v.02'),\n",
       " Synset('carry.v.04'),\n",
       " Synset('express.v.04'),\n",
       " Synset('express.v.05'),\n",
       " Synset('press_out.v.03'),\n",
       " Synset('express.v.07')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try a verb\n",
    "example_verb = 'expressing'\n",
    "synsets = wn.synsets(example_verb)\n",
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articulate; either verbally or with a cry, shout, or noise\n",
      "['She expressed her anger', 'He uttered a curse']\n",
      "['express', 'verbalize', 'verbalise', 'utter', 'give_tongue_to']\n"
     ]
    }
   ],
   "source": [
    "# same as before, but with a verb\n",
    "synset = synsets[1]\n",
    "\n",
    "# grab definition, usage examples, and lemmas\n",
    "print(synset.definition())\n",
    "print(synset.examples())\n",
    "print(synset.lemma_names())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet's verb organization is similar to that of nouns, but with a few differences. First, verbs are not organized into a hierarchy. Instead, they are organized into a directed acyclic graph (DAG). This means that verbs can have multiple parents, and multiple children. Second, verbs are organized into frames, which are groups of verbs that share similar meanings. Each frame has a unique name, and a unique offset. The offset is the frame's position in the WordNet database file. The name is a concatenation of the frame's offset, and the number of the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "express\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# run the verb through morphy to get all possible verb forms\n",
    "print(wn.morphy(example_verb, wn.NOUN))\n",
    "print(wn.morphy(example_verb, wn.VERB))\n",
    "print(wn.morphy(example_verb, wn.ADJ))\n",
    "print(wn.morphy(example_verb, wn.ADV))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('complect.v.01'), Synset('interconnect.v.02')]\n",
      "[Synset('network.n.01'), Synset('network.n.02'), Synset('net.n.06'), Synset('network.n.04'), Synset('network.n.05'), Synset('network.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# grab two words that are similar\n",
    "word1 = 'interlinked'\n",
    "word2 = 'network'\n",
    "\n",
    "# get the synsets for each word\n",
    "synsets1 = wn.synsets(word1)\n",
    "synsets2 = wn.synsets(word2)\n",
    "\n",
    "print(synsets1)\n",
    "print(synsets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n",
      "0.2222222222222222\n",
      "Synset('interconnect.v.02')\n",
      "Synset('network.v.01')\n"
     ]
    }
   ],
   "source": [
    "#wu-palmer similarity\n",
    "print(synsets1[1].wup_similarity(synsets2[0]))\n",
    "print(synsets2[0].wup_similarity(synsets1[1]))\n",
    "\n",
    "# lesk similarity\n",
    "from nltk.wsd import lesk\n",
    "print(lesk(text, word1))\n",
    "print(lesk(text, word2))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Similarity in this case seems to be symmetric. If `a` is similar to `b`, then `b` is similar to `a`. This is not always the case, but it is in this case.\n",
    "\n",
    "The lesk similarity was largely able to capture the meaning in this case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiwordnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiwordnet is a database of words and their associated sentiment. Each word is associated with a positive score, a negative score, and an objective score. The positive score is the probability that the word is positive. The negative score is the probability that the word is negative. The objective score is the probability that the word is objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hate.n.01: PosScore=0.125 NegScore=0.375>\n",
      "0.125\n",
      "0.375\n",
      "0.5\n",
      "<hate.v.01: PosScore=0.0 NegScore=0.75>\n",
      "0.0\n",
      "0.75\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# an example of a word that is emotionally charged\n",
    "word = 'hate'\n",
    "synsets = wn.synsets(word)\n",
    "\n",
    "# create a sentisynset for each synset\n",
    "sentisysnsets = []\n",
    "for synset in synsets:\n",
    "    sentisysnsets.append(swn.senti_synset(synset.name()))\n",
    "\n",
    "# print the positive and negative scores for each synset\n",
    "for sentisynset in sentisysnsets:\n",
    "    print(sentisynset)\n",
    "    print(sentisynset.pos_score())\n",
    "    print(sentisynset.neg_score())\n",
    "    print(sentisynset.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tPositive\tNegative\tObjective\n",
      "are\t0.0\t0.0\t1.0\n",
      "a\t0.0\t0.0\t1.0\n",
      "very\t0.5\t0.0\t0.5\n",
      "nice\t0.0\t0.0\t1.0\n",
      "person\t0.0\t0.0\t1.0\n",
      "I\t0.0\t0.0\t1.0\n",
      "enjoy\t0.375\t0.0\t0.625\n",
      "being\t0.0\t0.0\t1.0\n",
      "around\t0.0\t0.0\t1.0\n"
     ]
    }
   ],
   "source": [
    "example_sentence = 'You are a very nice person and I enjoy being around you.'\n",
    "\n",
    "# tokenize the sentence\n",
    "tokens = nltk.word_tokenize(example_sentence)\n",
    "\n",
    "# find the polarity of each word in the sentence\n",
    "print('Word\\tPositive\\tNegative\\tObjective')\n",
    "for token in tokens:\n",
    "    synsets = wn.synsets(token)\n",
    "    if synsets:\n",
    "        sentisynset = swn.senti_synset(synsets[0].name())\n",
    "        print(token + '\\t' + str(sentisynset.pos_score()) + '\\t' + str(sentisynset.neg_score()) + '\\t' + str(sentisynset.obj_score()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores could be used for sentiment analysis of input text and, importantly, could also be used for tonal matching of response text. I expect these would be especially useful in stock market analysis, where the sentiment of a company's stock could be used to predict its future performance, and a trading bot could rapidly respond to changes in sentiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collocation is a sequence of words that occur together more often than would be expected by chance. Some examples of collocations are \"New York\", \"United States\", and \"United Kingdom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "# import text4 from nltk book\n",
    "from nltk.book import text4\n",
    "\n",
    "# find collocations in text4\n",
    "collocations = text4.collocations()\n",
    "collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10025\n",
      "p(United States) =  0.015860349127182045\n",
      "p(United) =  0.0170573566084788\n",
      "p(States) =  0.03301745635910224\n",
      "pmi =  4.815657649820885\n"
     ]
    }
   ],
   "source": [
    "# calculate mutual information for the 'United States' collocation\n",
    "import math\n",
    "text = ' '.join(text4.tokens)\n",
    "vocab = len(set(text4))\n",
    "print(vocab)\n",
    "hg = text.count('United States')/vocab\n",
    "print(\"p(United States) = \",hg )\n",
    "h = text.count('United')/vocab\n",
    "print(\"p(United) = \", h)\n",
    "g = text.count('States')/vocab\n",
    "print('p(States) = ', g)\n",
    "pmi = math.log2(hg / (h * g))\n",
    "print('pmi = ', pmi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher pmi score indicates a higher likelihood that the words are a collocation. What seems most interesting to myself though is that the \"States\" has a much higher frequency than \"United\", which indicates that \"United\" is frequently used with \"States\", but not the other way around. For the purposes of NLP, this could be useful for determining the most important words in a sentence, and for determining the most important words in a document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4533960412f148fa378cbcde7564d079cd30a0b0b92f06b3b6550f76c759543d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
